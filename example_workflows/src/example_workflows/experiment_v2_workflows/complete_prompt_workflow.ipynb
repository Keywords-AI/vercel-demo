{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Workflow Test - Experiment V2\n",
        "\n",
        "This notebook demonstrates the complete prompt workflow using HTTP requests.\n",
        "\n",
        "**CRITICAL**: This test creates logs with VARIABLES in the input field (not messages!)\n",
        "because prompt workflows need variables to render prompts.\n",
        "\n",
        "## Test Coverage:\n",
        "1. Create and deploy a prompt with Jinja2 template\n",
        "2. Create logs with variables using the deployed prompt\n",
        "3. Create dataset from those logs\n",
        "4. Create experiment with prompt workflow\n",
        "5. Verify workflow execution\n",
        "6. Verify span tree structure (root + workflow + evaluator spans)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "**Before running this notebook**, set your API key using one of these methods:\n",
        "\n",
        "### Option 1: Create a `.env` file (Recommended)\n",
        "Create a `.env` file in the project root with:\n",
        "```\n",
        "KEYWORDSAI_API_KEY=your-api-key-here\n",
        "```\n",
        "\n",
        "### Option 2: Export environment variable\n",
        "```bash\n",
        "export KEYWORDSAI_API_KEY=\"your-api-key-here\"\n",
        "```\n",
        "\n",
        "The cell below will load the API key from the environment/`.env` file and validate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded .env from /Users/huangyunrui/platform/keywordsai-example-projects/example_workflows/src/example_workflows/experiment_v2_workflows/.env: True\n",
            "‚úÖ API Key loaded: s1SiVa3y********************\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "loaded = load_dotenv(override=True)\n",
        "\n",
        "# API Configuration\n",
        "BASE_URL = os.getenv(\"KEYWORDSAI_BASE_URL\")\n",
        "API_KEY = os.getenv(\"KEYWORDSAI_API_KEY\")\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\n",
        "        \"‚ùå KEYWORDSAI_API_KEY environment variable not set!\\n\"\n",
        "        \"Make sure you have a .env file with: KEYWORDSAI_API_KEY=your-api-key-here\\n\"\n",
        "        \"Or set it with: export KEYWORDSAI_API_KEY='your-api-key-here'\"\n",
        "    )\n",
        "\n",
        "print(f\"Loaded .env from {os.getcwd()}/.env: {loaded}\")\n",
        "print(f\"‚úÖ API Key loaded: {API_KEY[:8]}{'*' * 20}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def print_step(step_number: int, title: str):\n",
        "    \"\"\"Print a formatted step header.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"STEP {step_number}: {title}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "def print_success(message: str):\n",
        "    \"\"\"Print a success message.\"\"\"\n",
        "    print(f\"‚úÖ {message}\")\n",
        "\n",
        "def print_warning(message: str):\n",
        "    \"\"\"Print a warning message.\"\"\"\n",
        "    print(f\"‚ö†Ô∏è  {message}\")\n",
        "\n",
        "def print_error(message: str):\n",
        "    \"\"\"Print an error message.\"\"\"\n",
        "    print(f\"‚ùå {message}\")\n",
        "\n",
        "def print_info(message: str):\n",
        "    \"\"\"Print an info message.\"\"\"\n",
        "    print(f\"‚ÑπÔ∏è  {message}\")\n",
        "\n",
        "def wait_for_processing(seconds: int = 15):\n",
        "    \"\"\"Wait for async processing to complete.\"\"\"\n",
        "    print(f\"\\n‚è≥ Waiting {seconds} seconds for processing...\")\n",
        "    time.sleep(seconds)\n",
        "    print(\"‚úì Wait complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track resources created throughout the workflow\n",
        "prompt_id = None\n",
        "log_ids = []\n",
        "dataset_id = None\n",
        "evaluator_slug = None\n",
        "experiment_id = None\n",
        "count = 0\n",
        "success = False\n",
        "\n",
        "# Headers for API requests\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Create and Deploy Prompt\n",
        "\n",
        "Create a prompt with a Jinja2 template and deploy it.\n",
        "\n",
        "**Key Insight**: To deploy a prompt version:\n",
        "1. Create version 1 (draft, readonly=False)\n",
        "2. Create version 2 (this makes version 1 readonly=True)\n",
        "3. Deploy version 1 (only readonly versions can be deployed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 0: Create and Deploy Prompt\n",
            "======================================================================\n",
            "\n",
            "üìù Creating prompt...\n",
            "‚úÖ Prompt created with ID: 1a716b85c23d453d976054509b5426a1\n"
          ]
        }
      ],
      "source": [
        "print_step(0, \"Create and Deploy Prompt\")\n",
        "\n",
        "# Step 0a: Create the prompt\n",
        "print(\"\\nüìù Creating prompt...\")\n",
        "prompt_data = {\n",
        "    \"name\": \"Prompt workflow test\",\n",
        "    \"description\": \"Prompt for testing prompt workflow experiments\"\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{BASE_URL}/prompts/\",\n",
        "    headers=headers,\n",
        "    json=prompt_data\n",
        ")\n",
        "response.raise_for_status()\n",
        "\n",
        "prompt_result = response.json()\n",
        "prompt_id = prompt_result.get('id')\n",
        "print_success(f\"Prompt created with ID: {prompt_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Creating prompt version 1...\n",
            "‚úÖ Prompt version 1 created (draft)\n"
          ]
        }
      ],
      "source": [
        "# Step 0b: Create version 1 with Jinja2 template\n",
        "print(\"\\nüìù Creating prompt version 1...\")\n",
        "\n",
        "system_content = \"\"\"{# This is a comment showing personal information #}\n",
        "\n",
        "Height: {{ height }}v\n",
        "\n",
        "Weight: {{ weight | float }}  {# Filter example #}\n",
        "\n",
        "Hobby: {{ hobby | title }}    {# Makes first letter uppercase #}\n",
        "\n",
        "Name: {{ name | default('Anonymous') }}  {# Default value if name is undefined #}\n",
        "\n",
        "First Name: {{ full_name.first_name }}\n",
        "\n",
        "Last Name: {{ full_name.last_name }}\n",
        "\n",
        "Full Name: {{ full_name.first_name ~ ' ' ~ full_name.last_name }}  {# String concatenation #}\n",
        "\n",
        "Age: {{ age | int }}         {# Type conversion #}\n",
        "\n",
        "\n",
        "\n",
        "{# Math operations #}\n",
        "\n",
        "Years until retirement: {{ 65 - age }}\n",
        "\n",
        "Monthly salary in USD: {{ salary | round(2) }}\n",
        "\n",
        "\n",
        "\n",
        "The company position is shown below:\n",
        "\n",
        "{% if name == 'Raymond' %}\n",
        "\n",
        "    He is the Co-founder & CTO\n",
        "\n",
        "{% elif name == 'Andy' %}\n",
        "\n",
        "    He is the Co-founder & CEO\n",
        "\n",
        "{% elif name == 'Hendrix' %}\n",
        "\n",
        "    He is the Co-founder & CPO\n",
        "\n",
        "{% else %}\n",
        "\n",
        "    He is not a member of Keywords AI\n",
        "\n",
        "{% endif %}\n",
        "\n",
        "\n",
        "\n",
        "{# Using set to create variables #}\n",
        "\n",
        "{% set years_of_experience = 5 %}\n",
        "\n",
        "Experience: {{ years_of_experience }} years\n",
        "\n",
        "\n",
        "\n",
        "He has some items in his backpack, they are listed below:\n",
        "\n",
        "\n",
        "\n",
        "{# Loop with counter and conditions #}\n",
        "\n",
        "{% for item in backpack_items %}\n",
        "\n",
        "    {{ loop.index }}. {{ item }}\n",
        "\n",
        "    {% if loop.first %}(This is the first item){% endif %}\n",
        "\n",
        "    {% if loop.last %}(This is the last item){% endif %}\n",
        "\n",
        "    {% if not loop.last %}, {% endif %}\n",
        "\n",
        "{% else %}\n",
        "\n",
        "    No items in backpack!\n",
        "\n",
        "{% endfor %}\n",
        "\n",
        "\n",
        "\n",
        "{# Dictionary iteration #}\n",
        "\n",
        "Skills:\n",
        "\n",
        "{% for skill, level in skills.items() %}\n",
        "\n",
        "    - {{ skill }}: {{ level }}/10\n",
        "\n",
        "{% endfor %}\n",
        "\n",
        "\n",
        "\n",
        "{# Using macros (reusable template snippets) #}\n",
        "\n",
        "{% macro render_achievement(title, year) %}\n",
        "\n",
        "    {{ year }}: {{ title }}\n",
        "\n",
        "{% endmacro %}\n",
        "\n",
        "\n",
        "\n",
        "Achievements:\n",
        "\n",
        "{{ render_achievement('Graduated', 2020) }}\n",
        "\n",
        "{{ render_achievement('Started Company', 2022) }}\n",
        "\n",
        "\n",
        "\n",
        "{# Working with dates #}\n",
        "\n",
        "Current Date: {{ now }}\n",
        "\n",
        "Formatted Date: {{ now }}\n",
        "\n",
        "\n",
        "\n",
        "{# String operations #}\n",
        "\n",
        "Email Domain: {{ email.split('@')[1] }}\n",
        "\n",
        "Username: {{ email | replace('@keywords.ai', '') }}\n",
        "\n",
        "\n",
        "\n",
        "{# Conditional assignment #}\n",
        "\n",
        "Experience Level: {{ 'Senior' if years_of_experience >= 5 else 'Junior' }}\n",
        "\n",
        "\n",
        "\n",
        "{# Length filter #}\n",
        "\n",
        "Number of Skills: {{ skills | length }}\n",
        "\n",
        "\n",
        "\n",
        "{# Join filter for arrays #}\n",
        "\n",
        "All Skills: {{ skills.keys() | join(', ') }}\n",
        "\n",
        "\n",
        "\n",
        "{# Using namespace for organization #}\n",
        "\n",
        "{% set ns = namespace(total_score=0) %}\n",
        "\n",
        "{% for score in scores %}\n",
        "\n",
        "    {% set ns.total_score = ns.total_score + score %}\n",
        "\n",
        "{% endfor %}\n",
        "\n",
        "Average Score: {{ (ns.total_score / scores|length) | round(2) }}\"\"\"\n",
        "\n",
        "version_data = {\n",
        "    \"description\": \"Test version with Jinja2 template for prompt workflow\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_content\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How many things are there in the backpack?\"\n",
        "        }\n",
        "    ],\n",
        "    \"model\": \"gpt-4o-mini\",\n",
        "    \"stream\": False,\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256,\n",
        "    \"variables\": {\n",
        "        \"name\": \"string\",\n",
        "        \"full_name\": \"object\",\n",
        "        \"age\": \"number\",\n",
        "        \"height\": \"string\",\n",
        "        \"weight\": \"number\",\n",
        "        \"hobby\": \"string\",\n",
        "        \"salary\": \"number\",\n",
        "        \"email\": \"string\",\n",
        "        \"now\": \"string\",\n",
        "        \"backpack_items\": \"array\",\n",
        "        \"skills\": \"object\",\n",
        "        \"scores\": \"array\",\n",
        "        \"customer_inquiry\": \"string\"\n",
        "    },\n",
        "    \"readonly\": True\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{BASE_URL}/prompts/{prompt_id}/versions/\",\n",
        "    headers=headers,\n",
        "    json=version_data\n",
        ")\n",
        "response.raise_for_status()\n",
        "\n",
        "version_result = response.json()\n",
        "version_number = version_result.get('version')\n",
        "print_success(f\"Prompt version {version_number} created (draft)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Creating version 2 to commit version 1...\n",
            "‚úÖ Version 2 created (version 1 is now readonly/committed)\n",
            "\n",
            "üìù Deploying prompt version 1...\n",
            "‚úÖ Version 1 deployed successfully\n",
            "‚ÑπÔ∏è  Using prompt ID: 1a716b85c23d453d976054509b5426a1\n"
          ]
        }
      ],
      "source": [
        "# Step 0c: Create version 2 to commit version 1\n",
        "print(\"\\nüìù Creating version 2 to commit version 1...\")\n",
        "dummy_version_data = {\n",
        "    \"description\": \"Dummy version to commit version 1\",\n",
        "    \"messages\": version_data[\"messages\"],\n",
        "    \"model\": version_data[\"model\"],\n",
        "    \"stream\": False,\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{BASE_URL}/prompts/{prompt_id}/versions/\",\n",
        "    headers=headers,\n",
        "    json=dummy_version_data\n",
        ")\n",
        "response.raise_for_status()\n",
        "print_success(\"Version 2 created (version 1 is now readonly/committed)\")\n",
        "\n",
        "# Step 0d: Deploy version 1\n",
        "print(\"\\nüìù Deploying prompt version 1...\")\n",
        "response = requests.patch(\n",
        "    f\"{BASE_URL}/prompts/{prompt_id}/versions/1/\",\n",
        "    headers=headers,\n",
        "    json={\"deploy\": True}\n",
        ")\n",
        "response.raise_for_status()\n",
        "\n",
        "deploy_result = response.json()\n",
        "print_success(f\"Version 1 deployed successfully\")\n",
        "print_info(f\"Using prompt ID: {prompt_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create Logs with Variables\n",
        "\n",
        "Create logs using the chat completions API with the deployed prompt.\n",
        "The logs contain **variables** that will be used to render the prompt template.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 1: Create Logs with Variables via Chat Completions API\n",
            "======================================================================\n",
            "\n",
            "Using custom identifier: prompt_workflow_test\n",
            "\n",
            "Creating log 1/3...\n",
            "‚úÖ Completion created with ID: chatcmpl-CnvcT9neGOmnirsgiOcdX0dnVZlys\n",
            "\n",
            "Creating log 2/3...\n",
            "‚úÖ Completion created with ID: chatcmpl-CnvcUr6VUfkohuXV3byWc4LCjLo5D\n",
            "\n",
            "Creating log 3/3...\n",
            "‚úÖ Completion created with ID: chatcmpl-CnvcVwRQV38YyN6Q6FLXcNP5feBLD\n",
            "‚úÖ \n",
            "Created 3 completions total\n",
            "‚ÑπÔ∏è  All logs tagged with custom_identifier: prompt_workflow_test\n",
            "‚ÑπÔ∏è  Note: Completion IDs are different from log unique_ids\n",
            "‚ÑπÔ∏è  We'll fetch actual log IDs in the next step...\n",
            "\n",
            "‚è≥ Waiting 30 seconds for processing...\n",
            "‚úì Wait complete\n"
          ]
        }
      ],
      "source": [
        "print_step(1, \"Create Logs with Variables via Chat Completions API\")\n",
        "\n",
        "test_variables = [\n",
        "    {\n",
        "        \"height\": \"6 foot 2\", \"weight\": 180, \"hobby\": \"Aviation\", \"name\": \"Raymond\",\n",
        "        \"full_name\": {\"first_name\": \"Raymond\", \"last_name\": \"Huang\"},\n",
        "        \"age\": 28, \"salary\": 120000.50, \"email\": \"raymond@keywords.ai\",\n",
        "        \"backpack_items\": [\"Laptop\", \"Notebook\", \"Pen\"],\n",
        "        \"skills\": {\"Python\": 9, \"Django\": 8, \"React\": 7},\n",
        "        \"now\": \"2025-11-30\", \"scores\": [85, 90, 88],\n",
        "        \"customer_inquiry\": \"My order is damaged\"\n",
        "    },\n",
        "    {\n",
        "        \"height\": \"5 foot 10\", \"weight\": 165, \"hobby\": \"Reading\", \"name\": \"Alice\",\n",
        "        \"full_name\": {\"first_name\": \"Alice\", \"last_name\": \"Johnson\"},\n",
        "        \"age\": 32, \"salary\": 95000.00, \"email\": \"alice@keywords.ai\",\n",
        "        \"backpack_items\": [\"Book\", \"Water bottle\"],\n",
        "        \"skills\": {\"JavaScript\": 8, \"CSS\": 9},\n",
        "        \"now\": \"2025-11-30\", \"scores\": [92, 88, 95],\n",
        "        \"customer_inquiry\": \"Wrong item received\"\n",
        "    },\n",
        "    {\n",
        "        \"height\": \"5 foot 8\", \"weight\": 155, \"hobby\": \"Gaming\", \"name\": \"Bob\",\n",
        "        \"full_name\": {\"first_name\": \"Bob\", \"last_name\": \"Smith\"},\n",
        "        \"age\": 25, \"salary\": 75000.00, \"email\": \"bob@keywords.ai\",\n",
        "        \"backpack_items\": [\"Controller\", \"Headset\", \"Charger\"],\n",
        "        \"skills\": {\"Java\": 7, \"SQL\": 8, \"AWS\": 6},\n",
        "        \"now\": \"2025-11-30\", \"scores\": [78, 85, 80],\n",
        "        \"customer_inquiry\": \"Delayed delivery\"\n",
        "    }\n",
        "]\n",
        "\n",
        "test_run_id = int(time.time())\n",
        "custom_identifier = f\"prompt_workflow_test\"  # Single identifier for all logs\n",
        "completion_ids = []\n",
        "\n",
        "print(f\"\\nUsing custom identifier: {custom_identifier}\")\n",
        "\n",
        "for i, variables in enumerate(test_variables, 1):\n",
        "    payload = {\n",
        "        \"model\": \"gpt-4o-mini\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"placeholder\"}],\n",
        "        \"prompt\": {\"prompt_id\": prompt_id, \"variables\": variables, \"override\": True},\n",
        "        \"custom_identifier\": custom_identifier  # Same identifier for all logs\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nCreating log {i}/3...\")\n",
        "    response = requests.post(f\"{BASE_URL}/chat/completions\", headers=headers, json=payload)\n",
        "    \n",
        "    if response.status_code in [200, 201]:\n",
        "        result = response.json()\n",
        "        completion_id = result.get(\"id\")\n",
        "        print_success(f\"Completion created with ID: {completion_id}\")\n",
        "        completion_ids.append(completion_id)\n",
        "    else:\n",
        "        print_error(f\"Failed: {response.status_code}\")\n",
        "        print(f\"Response: {response.text[:300]}\")\n",
        "\n",
        "print_success(f\"\\nCreated {len(completion_ids)} completions total\")\n",
        "print_info(f\"All logs tagged with custom_identifier: {custom_identifier}\")\n",
        "print_info(\"Note: Completion IDs are different from log unique_ids\")\n",
        "print_info(\"We'll fetch actual log IDs in the next step...\")\n",
        "wait_for_processing(30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1b: Fetch Actual Log IDs\n",
        "\n",
        "The completion IDs from `/chat/completions` are different from the log `unique_id`.\n",
        "We need to fetch the actual log IDs using the custom_identifier filter.\n",
        "\n",
        "**Note:** We query logs from the last 30 minutes to ensure we capture all recently created logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìã Fetching actual log IDs using custom_identifier filter...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3h/z7vpqhz947d5ckx70zcxmh080000gn/T/ipykernel_49403/2483804108.py:4: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  end_time = datetime.utcnow()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fetched 6 actual log IDs:\n",
            "  1. 5648f92f14574bd6b26c839d9b8066d1 (custom_id: prompt_workflow_test)\n",
            "  2. 38f6b84fc2c64c3d97b37bc3cb229c1e (custom_id: prompt_workflow_test)\n",
            "  3. 8af32d8012d44180ad1d97a3c15bb032 (custom_id: prompt_workflow_test)\n",
            "  4. 99cdb8b1f2d64fa882f4c73b61b2297d (custom_id: prompt_workflow_test)\n",
            "  5. 3e60f218e88b489eb406533a86bccc19 (custom_id: prompt_workflow_test)\n",
            "  6. 3a41c7cbad564e389f7a08aed1372b80 (custom_id: prompt_workflow_test)\n",
            "‚ö†Ô∏è  Expected 3 logs but found 6\n",
            "‚ÑπÔ∏è  Some logs may still be processing...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìã Fetching actual log IDs using custom_identifier filter...\")\n",
        "\n",
        "# Set time range: last 30 minutes\n",
        "end_time = datetime.utcnow()\n",
        "start_time = end_time - timedelta(minutes=30)\n",
        "\n",
        "params = {\n",
        "    \"page\": 1,\n",
        "    \"page_size\": 100,\n",
        "    \"start_time\": start_time.isoformat() + \"Z\",\n",
        "    \"end_time\": end_time.isoformat() + \"Z\",\n",
        "    \"all_envs\": \"true\"\n",
        "}\n",
        "\n",
        "filter_data = {\n",
        "    \"filters\": {\n",
        "        \"custom_identifier\": {\n",
        "            \"value\": [custom_identifier],\n",
        "            \"operator\": \"\",\n",
        "            \"connector\": \"AND\"\n",
        "        }\n",
        "    },\n",
        "    \"exporting\": False\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{BASE_URL}/request-logs/list/\",\n",
        "    headers=headers,\n",
        "    params=params,\n",
        "    json=filter_data\n",
        ")\n",
        "response.raise_for_status()\n",
        "\n",
        "logs_response = response.json()\n",
        "fetched_logs = logs_response.get('results', [])\n",
        "\n",
        "log_ids = []\n",
        "for log in fetched_logs:\n",
        "    log_id = log.get('unique_id') or log.get('id')\n",
        "    if log_id:\n",
        "        log_ids.append(log_id)\n",
        "\n",
        "print_success(f\"Fetched {len(log_ids)} actual log IDs:\")\n",
        "for i, log_id in enumerate(log_ids, 1):\n",
        "    custom_id = fetched_logs[i-1].get('custom_identifier', 'N/A')\n",
        "    print(f\"  {i}. {log_id} (custom_id: {custom_id})\")\n",
        "\n",
        "expected_count = len(test_variables)\n",
        "if len(log_ids) != expected_count:\n",
        "    print_warning(f\"Expected {expected_count} logs but found {len(log_ids)}\")\n",
        "    print_info(\"Some logs may still be processing...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 2: Create Dataset from Logs\n",
            "======================================================================\n",
            "\n",
            "Creating dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3h/z7vpqhz947d5ckx70zcxmh080000gn/T/ipykernel_49403/2900984637.py:3: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  end_time = datetime.utcnow()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset created with ID: ce03dc10-f6d5-4ace-a18a-a5461fb9d4c0\n",
            "‚ÑπÔ∏è  Waiting for dataset to populate...\n",
            "\n",
            "‚è≥ Waiting 20 seconds for processing...\n",
            "‚úì Wait complete\n"
          ]
        }
      ],
      "source": [
        "print_step(2, \"Create Dataset from Logs\")\n",
        "\n",
        "end_time = datetime.utcnow()\n",
        "start_time = end_time - timedelta(minutes=10)\n",
        "\n",
        "dataset_data = {\n",
        "    \"name\": f\"Prompt workflow test\",\n",
        "    \"description\": \"Dataset for prompt workflow notebook test\",\n",
        "    \"type\": \"sampling\",\n",
        "    \"start_time\": start_time.isoformat() + \"Z\",\n",
        "    \"end_time\": end_time.isoformat() + \"Z\",\n",
        "    \"initial_log_filters\": {\n",
        "        \"custom_identifier\": {\n",
        "            \"value\": [custom_identifier],\n",
        "            \"operator\": \"\",\n",
        "            \"connector\": \"AND\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nCreating dataset...\")\n",
        "response = requests.post(f\"{BASE_URL}/datasets\", headers=headers, json=dataset_data)\n",
        "response.raise_for_status()\n",
        "\n",
        "dataset_result = response.json()\n",
        "dataset_id = dataset_result.get('id')\n",
        "print_success(f\"Dataset created with ID: {dataset_id}\")\n",
        "print_info(\"Waiting for dataset to populate...\")\n",
        "wait_for_processing(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Evaluator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 3: Create Evaluator\n",
            "======================================================================\n",
            "\n",
            "Creating evaluator...\n",
            "  Name: Prompt workflow test\n",
            "  Slug: prompt_workflow_test_eval_1766015651\n",
            "  Type: llm\n",
            "  Score Type: numerical\n",
            "‚úÖ Evaluator created: prompt_workflow_test_eval_1766015651\n",
            "  Evaluator ID: 355967a2-2cfc-4abf-ad44-f1abaaf4ec49\n"
          ]
        }
      ],
      "source": [
        "print_step(3, \"Create Evaluator\")\n",
        "\n",
        "evaluator_slug = f\"prompt_workflow_test_eval_{int(time.time())}\"\n",
        "\n",
        "evaluator_data = {\n",
        "    \"name\": f\"Prompt workflow test\",\n",
        "    \"evaluator_slug\": evaluator_slug,\n",
        "    \"type\": \"llm\",\n",
        "    \"score_value_type\": \"numerical\",\n",
        "    \"description\": \"Evaluates response quality on a 1-5 scale\",\n",
        "    \"configurations\": {\n",
        "        \"evaluator_definition\": \"Rate the response quality based on accuracy, relevance, and completeness.\\n<llm_input>{{input}}</llm_input>\\n<llm_output>{{output}}</llm_output>\",\n",
        "        \"scoring_rubric\": \"1=Poor, 2=Fair, 3=Good, 4=Very Good, 5=Excellent\",\n",
        "        \"llm_engine\": \"gpt-4o-mini\",\n",
        "        \"model_options\": {\n",
        "            \"temperature\": 0.1,\n",
        "            \"max_tokens\": 200\n",
        "        },\n",
        "        \"min_score\": 1.0,\n",
        "        \"max_score\": 5.0,\n",
        "        \"passing_score\": 3.0\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nCreating evaluator...\")\n",
        "print(f\"  Name: {evaluator_data['name']}\")\n",
        "print(f\"  Slug: {evaluator_slug}\")\n",
        "print(f\"  Type: {evaluator_data['type']}\")\n",
        "print(f\"  Score Type: {evaluator_data['score_value_type']}\")\n",
        "\n",
        "response = requests.post(f\"{BASE_URL}/evaluators\", headers=headers, json=evaluator_data)\n",
        "response.raise_for_status()\n",
        "\n",
        "evaluator_result = response.json()\n",
        "evaluator_slug = evaluator_result.get('evaluator_slug')\n",
        "print_success(f\"Evaluator created: {evaluator_slug}\")\n",
        "print(f\"  Evaluator ID: {evaluator_result.get('id')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Experiment with Prompt Workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 4: Create Experiment with Prompt Workflow\n",
            "======================================================================\n",
            "\n",
            "Creating experiment with:\n",
            "  Prompt ID: 1a716b85c23d453d976054509b5426a1\n",
            "  Dataset ID: ce03dc10-f6d5-4ace-a18a-a5461fb9d4c0\n",
            "  Evaluator: prompt_workflow_test_eval_1766015651\n",
            "‚úÖ Experiment created with ID: cceb26eb951a4cdda9fb46d4eae592e6\n",
            "‚ÑπÔ∏è  Status: pending\n",
            "‚ÑπÔ∏è  Waiting for async workflow execution...\n",
            "\n",
            "‚è≥ Waiting 30 seconds for processing...\n",
            "‚úì Wait complete\n"
          ]
        }
      ],
      "source": [
        "print_step(4, \"Create Experiment with Prompt Workflow\")\n",
        "\n",
        "experiment_data = {\n",
        "    \"name\": f\"Prompt workflow test\",\n",
        "    \"description\": \"Testing prompt workflow from notebook\",\n",
        "    \"dataset_id\": dataset_id,\n",
        "    \"workflow\": [{\n",
        "        \"type\": \"prompt\",\n",
        "        \"config\": {\"prompt_id\": prompt_id}\n",
        "    }],\n",
        "    \"evaluator_slugs\": [evaluator_slug]\n",
        "}\n",
        "\n",
        "print(f\"\\nCreating experiment with:\")\n",
        "print(f\"  Prompt ID: {prompt_id}\")\n",
        "print(f\"  Dataset ID: {dataset_id}\")\n",
        "print(f\"  Evaluator: {evaluator_slug}\")\n",
        "\n",
        "response = requests.post(f\"{BASE_URL}/v2/experiments/\", headers=headers, json=experiment_data)\n",
        "response.raise_for_status()\n",
        "\n",
        "experiment_result = response.json()\n",
        "experiment_id = experiment_result.get('id')\n",
        "print_success(f\"Experiment created with ID: {experiment_id}\")\n",
        "print_info(f\"Status: {experiment_result.get('status')}\")\n",
        "print_info(\"Waiting for async workflow execution...\")\n",
        "wait_for_processing(30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify Workflow Execution\n",
        "\n",
        "Check that the experiment created logs successfully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 5: Verify Workflow Execution\n",
            "======================================================================\n",
            "\n",
            "Listing experiment logs...\n",
            "\n",
            "üìä Found 3 log(s)\n",
            "üìà Status breakdown: {'success': 3}\n",
            "‚úÖ Workflow executed successfully!\n"
          ]
        }
      ],
      "source": [
        "print_step(5, \"Verify Workflow Execution\")\n",
        "\n",
        "print(\"\\nListing experiment logs...\")\n",
        "response = requests.get(\n",
        "    f\"{BASE_URL}/v2/experiments/{experiment_id}/logs/list/\",\n",
        "    headers=headers\n",
        ")\n",
        "response.raise_for_status()\n",
        "\n",
        "logs_result = response.json()\n",
        "logs = logs_result.get('results', [])\n",
        "count = len(logs)\n",
        "\n",
        "print(f\"\\nüìä Found {count} log(s)\")\n",
        "\n",
        "if count == 0:\n",
        "    print_error(\"No logs created - workflow may have failed!\")\n",
        "    print_info(\"Check:\")\n",
        "    print(\"  1. Celery worker is running\")\n",
        "    print(\"  2. Prompt is properly deployed\")\n",
        "    print(\"  3. Dataset contains logs\")\n",
        "else:\n",
        "    # Check log status\n",
        "    statuses = {}\n",
        "    for log in logs:\n",
        "        status = log.get('status', 'unknown')\n",
        "        statuses[status] = statuses.get(status, 0) + 1\n",
        "    \n",
        "    print(f\"üìà Status breakdown: {statuses}\")\n",
        "    print_success(f\"Workflow executed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Verify Span Tree Structure\n",
        "\n",
        "Verify that the span tree has the expected structure for a prompt workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 6: Verify Span Tree Structure\n",
            "======================================================================\n",
            "\n",
            "Getting span tree for log: 49ae2c09bf48c2751b131ff5274b879d\n",
            "\n",
            "üå≤ Span tree contains 1 top-level span(s):\n",
            "\n",
            "üå≤ Nested Span Tree Structure:\n",
            "- experiment_trace (span_type: N/A, log_type: workflow)\n",
            "  - workflow_execution (span_type: N/A, log_type: chat)\n",
            "    - Experiment Workflow.prompt (span_type: N/A, log_type: workflow)\n",
            "      - workflow.prompt.load_prompt (span_type: N/A, log_type: workflow)\n",
            "      - workflow.prompt.completion (span_type: N/A, log_type: chat)\n",
            "  - evaluator.prompt_workflow_test_eval_1766015651 (span_type: N/A, log_type: score)\n",
            "\n",
            "üìã Verification:\n",
            "  Root span (experiment_trace): ‚úÖ\n",
            "  Workflow execution span: ‚úÖ\n",
            "  Prompt workflow span: ‚úÖ\n",
            "  ‚îú‚îÄ load_prompt child span: ‚úÖ\n",
            "  ‚îú‚îÄ completion child span: ‚úÖ\n",
            "  ‚îî‚îÄ LLM call span (log_type=llm): ‚ùå (REQUIRED)\n",
            "  Evaluator span(s): ‚úÖ (REQUIRED)\n",
            "\n",
            "üîç Evaluator Details (1 span(s)):\n",
            "  [1] evaluator.prompt_workflow_test_eval_1766015651\n",
            "      Status: success\n",
            "‚ùå \n",
            "‚ùå Span tree missing expected spans\n",
            "‚ö†Ô∏è    Missing LLM call - workflow may not have executed properly\n"
          ]
        }
      ],
      "source": [
        "print_step(6, \"Verify Span Tree Structure\")\n",
        "\n",
        "if not logs:\n",
        "    print_error(\"No logs to verify\")\n",
        "else:\n",
        "    # Get detailed span tree for first log\n",
        "    log_id = logs[0].get('id')\n",
        "    print(f\"\\nGetting span tree for log: {log_id}\")\n",
        "    \n",
        "    response = requests.get(\n",
        "        f\"{BASE_URL}/v2/experiments/{experiment_id}/logs/{log_id}/\",\n",
        "        headers=headers,\n",
        "        params={\"detail\": 1}\n",
        "    )\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    log_detail = response.json()\n",
        "    span_tree = log_detail.get('span_tree', [])\n",
        "    \n",
        "    print(f\"\\nüå≤ Span tree contains {len(span_tree)} top-level span(s):\")\n",
        "    \n",
        "    # Collect all spans (including nested)\n",
        "    all_spans = []\n",
        "    \n",
        "    def collect_spans(spans):\n",
        "        for span in spans:\n",
        "            all_spans.append(span)\n",
        "            if 'children' in span:\n",
        "                collect_spans(span['children'])\n",
        "    \n",
        "    collect_spans(span_tree)\n",
        "    \n",
        "    # Print span tree with indentation to show nesting\n",
        "    def print_span_tree(spans, indent=0):\n",
        "        for span in spans:\n",
        "            span_name = span.get('span_name', '')\n",
        "            span_type = span.get('span_type', 'N/A')\n",
        "            log_type = span.get('log_type', 'N/A')\n",
        "            prefix = \"  \" * indent + \"- \"\n",
        "            print(f\"{prefix}{span_name} (span_type: {span_type}, log_type: {log_type})\")\n",
        "            \n",
        "            # Print children if they exist\n",
        "            children = span.get('children', [])\n",
        "            if children:\n",
        "                print_span_tree(children, indent + 1)\n",
        "    \n",
        "    print(\"\\nüå≤ Nested Span Tree Structure:\")\n",
        "    print_span_tree(span_tree)\n",
        "    \n",
        "    # Verify expected spans\n",
        "    has_root = False\n",
        "    has_workflow = False\n",
        "    has_prompt_workflow = False\n",
        "    has_evaluator = False\n",
        "    has_llm_call = False\n",
        "    has_load_prompt = False\n",
        "    has_completion_span = False\n",
        "    evaluator_spans = []\n",
        "    llm_spans = []\n",
        "    \n",
        "    for span in all_spans:\n",
        "        span_name = span.get('span_name', '')\n",
        "        span_type = span.get('span_type', 'N/A')\n",
        "        log_type = span.get('log_type', 'N/A')\n",
        "        \n",
        "        if span_name == 'experiment_trace':\n",
        "            has_root = True\n",
        "        elif 'workflow_execution' in span_name:\n",
        "            has_workflow = True\n",
        "        elif 'prompt' in span_name.lower():\n",
        "            has_prompt_workflow = True\n",
        "        \n",
        "        # Check for prompt workflow child spans\n",
        "        if 'load_prompt' in span_name:\n",
        "            has_load_prompt = True\n",
        "        if 'completion' in span_name and 'workflow' in span_name:\n",
        "            has_completion_span = True\n",
        "        \n",
        "        # Check for LLM calls (actual generation)\n",
        "        if log_type == 'llm':\n",
        "            has_llm_call = True\n",
        "            llm_spans.append(span)\n",
        "        \n",
        "        # Check for evaluator spans (by span_type or name)\n",
        "        if span_type == 'SCORE' or 'evaluator' in span_name.lower():\n",
        "            has_evaluator = True\n",
        "            evaluator_spans.append(span)\n",
        "    \n",
        "    print(f\"\\nüìã Verification:\")\n",
        "    print(f\"  Root span (experiment_trace): {'‚úÖ' if has_root else '‚ùå'}\")\n",
        "    print(f\"  Workflow execution span: {'‚úÖ' if has_workflow else '‚ùå'}\")\n",
        "    print(f\"  Prompt workflow span: {'‚úÖ' if has_prompt_workflow else '‚ùå'}\")\n",
        "    print(f\"  ‚îú‚îÄ load_prompt child span: {'‚úÖ' if has_load_prompt else '‚ùå'}\")\n",
        "    print(f\"  ‚îú‚îÄ completion child span: {'‚úÖ' if has_completion_span else '‚ùå'}\")\n",
        "    print(f\"  ‚îî‚îÄ LLM call span (log_type=llm): {'‚úÖ' if has_llm_call else '‚ùå'} (REQUIRED)\")\n",
        "    print(f\"  Evaluator span(s): {'‚úÖ' if has_evaluator else '‚ùå'} (REQUIRED)\")\n",
        "    \n",
        "    if llm_spans:\n",
        "        print(f\"\\nüîç LLM Call Details ({len(llm_spans)} span(s)):\")\n",
        "        for i, llm_span in enumerate(llm_spans, 1):\n",
        "            print(f\"  [{i}] {llm_span.get('span_name', 'N/A')}\")\n",
        "            if 'model' in llm_span:\n",
        "                print(f\"      Model: {llm_span.get('model')}\")\n",
        "            if 'prompt_id' in llm_span:\n",
        "                print(f\"      Prompt ID: {llm_span.get('prompt_id')}\")\n",
        "            if 'prompt_tokens' in llm_span:\n",
        "                print(f\"      Prompt tokens: {llm_span.get('prompt_tokens')}\")\n",
        "            if 'completion_tokens' in llm_span:\n",
        "                print(f\"      Completion tokens: {llm_span.get('completion_tokens')}\")\n",
        "            if 'status' in llm_span:\n",
        "                print(f\"      Status: {llm_span.get('status')}\")\n",
        "    \n",
        "    if evaluator_spans:\n",
        "        print(f\"\\nüîç Evaluator Details ({len(evaluator_spans)} span(s)):\")\n",
        "        for i, eval_span in enumerate(evaluator_spans, 1):\n",
        "            print(f\"  [{i}] {eval_span.get('span_name', 'N/A')}\")\n",
        "            if 'evaluator_slug' in eval_span:\n",
        "                print(f\"      Evaluator slug: {eval_span.get('evaluator_slug')}\")\n",
        "            if 'score' in eval_span:\n",
        "                print(f\"      Score: {eval_span.get('score')}\")\n",
        "            if 'status' in eval_span:\n",
        "                print(f\"      Status: {eval_span.get('status')}\")\n",
        "    \n",
        "    success = has_root and has_workflow and has_prompt_workflow and has_llm_call and has_evaluator\n",
        "    \n",
        "    if success:\n",
        "        print_success(\"\\n‚úÖ Span tree structure is correct! All expected spans found.\")\n",
        "    else:\n",
        "        print_error(\"\\n‚ùå Span tree missing expected spans\")\n",
        "        if not has_llm_call:\n",
        "            print_warning(\"  Missing LLM call - workflow may not have executed properly\")\n",
        "            if not has_load_prompt and not has_completion_span:\n",
        "                print_warning(\"  ‚Üí Prompt workflow child spans are also missing!\")\n",
        "                print_info(\"  ‚Üí This suggests the prompt workflow didn't create child spans\")\n",
        "        if not has_evaluator:\n",
        "            print_warning(\"  Missing evaluator span - evaluators may still be running\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST SUMMARY\n",
            "======================================================================\n",
            "\n",
            "üìä Test Results:\n",
            "  Prompt ID: 1a716b85c23d453d976054509b5426a1\n",
            "  Logs Created: 6\n",
            "  Dataset ID: ce03dc10-f6d5-4ace-a18a-a5461fb9d4c0\n",
            "  Evaluator: prompt_workflow_test_eval_1766015651\n",
            "  Experiment ID: cceb26eb951a4cdda9fb46d4eae592e6\n",
            "  Experiment Logs: 3\n",
            "\n",
            "‚ö†Ô∏è  Some tests failed or incomplete\n",
            "   - Span tree structure incorrect\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TEST SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìä Test Results:\")\n",
        "print(f\"  Prompt ID: {prompt_id}\")\n",
        "print(f\"  Logs Created: {len(log_ids)}\")\n",
        "print(f\"  Dataset ID: {dataset_id}\")\n",
        "print(f\"  Evaluator: {evaluator_slug}\")\n",
        "print(f\"  Experiment ID: {experiment_id}\")\n",
        "print(f\"  Experiment Logs: {count if 'count' in locals() else 0}\")\n",
        "\n",
        "if count > 0 and success:\n",
        "    print(\"\\nüéâ All tests passed!\")\n",
        "    print(\"\\n‚úÖ Prompt workflow executed successfully\")\n",
        "    print(\"‚úÖ Span tree structure is correct\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Some tests failed or incomplete\")\n",
        "    if count == 0:\n",
        "        print(\"   - Logs not created\")\n",
        "    if not success:\n",
        "        print(\"   - Span tree structure incorrect\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
