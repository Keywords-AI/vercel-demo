{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Workflow Verification - Self-Contained\n",
        "\n",
        "This notebook demonstrates and verifies the complete workflow as documented in `experiment_v2_workflow_implementation.md`.\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚      CUSTOM WORKFLOW - USER FLOW            â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "                START\n",
        "                  â†“\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  1. Create Experiment       â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â†“\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  2. Get Placeholder Traces  â”‚\n",
        "    â”‚     (with inputs)           â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â†“\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  3. Process with Your Code  â”‚\n",
        "    â”‚     (external)              â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â†“\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  4. Submit Results          â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â†“\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  5. View Final Results      â”‚\n",
        "    â”‚     (with scores)           â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â†“\n",
        "                 END\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your API key and base URL here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "loaded = load_dotenv(override=True)\n",
        "\n",
        "# API Configuration\n",
        "BASE_URL = os.getenv(\"KEYWORDSAI_BASE_URL\")\n",
        "API_KEY = os.getenv(\"KEYWORDSAI_API_KEY\")\n",
        "\n",
        "if not API_KEY:\n",
        "    raise ValueError(\n",
        "        \"âŒ KEYWORDSAI_API_KEY environment variable not set!\\n\"\n",
        "        \"Make sure you have a .env file with: KEYWORDSAI_API_KEY=your-api-key-here\\n\"\n",
        "        \"Or set it with: export KEYWORDSAI_API_KEY='your-api-key-here'\"\n",
        "    )\n",
        "\n",
        "print(f\"Loaded .env from {os.getcwd()}/.env: {loaded}\")\n",
        "print(f\"âœ… API Key loaded: {API_KEY[:8]}{'*' * 20}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and All Functions\n",
        "\n",
        "All necessary imports and function definitions in one place.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import Dict, Any, List, Optional\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# ============================================================================\n",
        "# CONSISTENT NAMING FOR ALL COMPONENTS - MAKES CLEANUP EASY!\n",
        "# ============================================================================\n",
        "WORKFLOW_NAME = \"Custom workflow test\"\n",
        "\n",
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def print_step(step_number: int, title: str):\n",
        "    \"\"\"Print a formatted step header.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"STEP {step_number}: {title}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "def print_success(message: str):\n",
        "    \"\"\"Print a success message.\"\"\"\n",
        "    print(f\"âœ… {message}\")\n",
        "\n",
        "def print_warning(message: str):\n",
        "    \"\"\"Print a warning message.\"\"\"\n",
        "    print(f\"âš ï¸  {message}\")\n",
        "\n",
        "def print_error(message: str):\n",
        "    \"\"\"Print an error message.\"\"\"\n",
        "    print(f\"âŒ {message}\")\n",
        "\n",
        "def print_info(message: str):\n",
        "    \"\"\"Print an info message.\"\"\"\n",
        "    print(f\"â„¹ï¸  {message}\")\n",
        "\n",
        "def wait_for_processing(seconds: int = 15):\n",
        "    \"\"\"Wait for async processing to complete.\"\"\"\n",
        "    print(f\"\\nWaiting {seconds} seconds for processing...\")\n",
        "    time.sleep(seconds)\n",
        "    print(\"âœ“ Wait complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# API FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def create_log(\n",
        "    model: str,\n",
        "    input_messages: List[Dict[str, str]],\n",
        "    output_message: Dict[str, str],\n",
        "    custom_identifier: Optional[str] = None,\n",
        "    span_name: Optional[str] = None,\n",
        "    **kwargs\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Create a new log entry in Keywords AI.\"\"\"\n",
        "    url = f\"{BASE_URL}/request-logs/create\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"input\": input_messages,\n",
        "        \"output\": output_message\n",
        "    }\n",
        "    \n",
        "    if custom_identifier:\n",
        "        payload[\"custom_identifier\"] = custom_identifier\n",
        "    if span_name:\n",
        "        payload[\"span_name\"] = span_name\n",
        "    \n",
        "    payload.update(kwargs)\n",
        "    \n",
        "    print(\"Creating log entry...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    print(f\"  Model: {model}\")\n",
        "    print(f\"  Input messages: {len(input_messages)}\")\n",
        "    if custom_identifier:\n",
        "        print(f\"  Custom identifier: {custom_identifier}\")\n",
        "    if span_name:\n",
        "        print(f\"  Span name: {span_name}\")\n",
        "    print(f\"  Request Body: {json.dumps(payload, indent=2)}\")\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    print(f\"\\nâœ“ Log created successfully\")\n",
        "    if 'unique_id' in data:\n",
        "        print(f\"  Log ID (unique_id): {data.get('unique_id')}\")\n",
        "    if 'id' in data:\n",
        "        print(f\"  Log ID: {data.get('id')}\")\n",
        "    if 'trace_id' in data:\n",
        "        print(f\"  Trace ID: {data.get('trace_id')}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def create_dataset(\n",
        "    name: str, \n",
        "    description: str = \"\", \n",
        "    dataset_type: str = \"sampling\",\n",
        "    sampling: int = 50,\n",
        "    start_time: Optional[str] = None,\n",
        "    end_time: Optional[str] = None,\n",
        "    initial_log_filters: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Create a new dataset from logs with sampling and filtering.\"\"\"\n",
        "    url = f\"{BASE_URL}/datasets\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"type\": dataset_type\n",
        "    }\n",
        "    \n",
        "    if sampling and not initial_log_filters:\n",
        "        payload[\"sampling\"] = sampling\n",
        "    \n",
        "    if start_time:\n",
        "        payload[\"start_time\"] = start_time\n",
        "    if end_time:\n",
        "        payload[\"end_time\"] = end_time\n",
        "        \n",
        "    if initial_log_filters:\n",
        "        payload[\"initial_log_filters\"] = initial_log_filters\n",
        "    \n",
        "    print(\"Creating dataset...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    print(f\"  Name: {name}\")\n",
        "    print(f\"  Type: {dataset_type}\")\n",
        "    print(f\"  Request Body: {json.dumps(payload, indent=2)}\")\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    print(f\"\\nâœ“ Dataset created successfully\")\n",
        "    print(f\"  Dataset ID: {data.get('id')}\")\n",
        "    print(f\"  Name: {data.get('name')}\")\n",
        "    print(f\"  Status: {data.get('status', 'N/A')}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def list_dataset_logs(dataset_id: str, page: int = 1, page_size: int = 100) -> Dict[str, Any]:\n",
        "    \"\"\"List logs from a specific dataset.\"\"\"\n",
        "    url = f\"{BASE_URL}/datasets/{dataset_id}/logs/list\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"page_size\": page_size\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nListing logs for dataset {dataset_id}...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    print(f\"  Method: GET\")\n",
        "    print(f\"  Params: {params}\")\n",
        "    \n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    results = data.get('results', [])\n",
        "    total_count = data.get('count', 0)\n",
        "    \n",
        "    print(f\"âœ“ Retrieved {len(results)} logs (page {page})\")\n",
        "    print(f\"  Total logs in dataset: {total_count}\")\n",
        "    \n",
        "    if results:\n",
        "        print(f\"\\n  ðŸ“‹ First log structure:\")\n",
        "        first_log = results[0]\n",
        "        print(f\"    Keys: {list(first_log.keys())}\")\n",
        "        if 'id' in first_log:\n",
        "            print(f\"    Log ID: {first_log.get('id')}\")\n",
        "        if 'input' in first_log:\n",
        "            input_preview = str(first_log.get('input'))[:100]\n",
        "            print(f\"    Input preview: {input_preview}...\")\n",
        "        if 'output' in first_log:\n",
        "            output_preview = str(first_log.get('output'))[:100]\n",
        "            print(f\"    Output preview: {output_preview}...\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def create_evaluator(\n",
        "    name: str,\n",
        "    evaluator_slug: str,\n",
        "    evaluator_type: str,\n",
        "    score_value_type: str,\n",
        "    description: str = \"\",\n",
        "    configurations: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Create a custom evaluator in Keywords AI.\"\"\"\n",
        "    url = f\"{BASE_URL}/evaluators\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"name\": name,\n",
        "        \"evaluator_slug\": evaluator_slug,\n",
        "        \"type\": evaluator_type,\n",
        "        \"score_value_type\": score_value_type,\n",
        "        \"description\": description\n",
        "    }\n",
        "    \n",
        "    if configurations:\n",
        "        payload[\"configurations\"] = configurations\n",
        "    \n",
        "    print(\"Creating evaluator...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    print(f\"  Name: {name}\")\n",
        "    print(f\"  Slug: {evaluator_slug}\")\n",
        "    print(f\"  Type: {evaluator_type}\")\n",
        "    print(f\"  Score Type: {score_value_type}\")\n",
        "    print(f\"  Request Body: {json.dumps(payload, indent=2)}\")\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    print(f\"\\nâœ“ Evaluator created successfully\")\n",
        "    if 'id' in data:\n",
        "        print(f\"  Evaluator ID: {data.get('id')}\")\n",
        "    if 'evaluator_slug' in data:\n",
        "        print(f\"  Evaluator Slug: {data.get('evaluator_slug')}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def create_experiment(name: str, description: str, dataset_id: str, \n",
        "                     workflow: List[Dict], evaluator_slugs: List[str]) -> Dict[str, Any]:\n",
        "    \"\"\"Create a new custom workflow experiment.\"\"\"\n",
        "    url = f\"{BASE_URL}/v2/experiments/\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"dataset_id\": dataset_id,\n",
        "        \"workflow\": workflow,\n",
        "        \"evaluator_slugs\": evaluator_slugs\n",
        "    }\n",
        "    \n",
        "    print(\"Creating custom workflow experiment...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    print(f\"  Name: {name}\")\n",
        "    print(f\"  Dataset: {dataset_id}\")\n",
        "    print(f\"  Evaluators: {', '.join(evaluator_slugs)}\")\n",
        "    print(f\"  Request Body: {json.dumps(payload, indent=2)}\")\n",
        "    \n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    print(f\"\\nâœ“ Experiment created with ID: {data.get('id')}\")\n",
        "    print(f\"  Status: {data.get('status')}\")\n",
        "    print(\"  Placeholder traces are being created asynchronously...\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def list_experiment_logs(exp_id: str, filters: Optional[Dict[str, Any]] = None, \n",
        "                        page: int = 1, page_size: int = 100) -> Dict[str, Any]:\n",
        "    \"\"\"List traces for an experiment with optional filtering.\"\"\"\n",
        "    url = f\"{BASE_URL}/v2/experiments/{exp_id}/logs/list/\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    params = {\n",
        "        \"page\": page,\n",
        "        \"page_size\": page_size\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nListing experiment logs for {exp_id}...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    print(f\"  Params: {params}\")\n",
        "    if filters:\n",
        "        print(f\"  Filters: {json.dumps(filters, indent=2)}\")\n",
        "    \n",
        "    try:\n",
        "        if filters:\n",
        "            request_body = {\"filters\": filters}\n",
        "            print(f\"  Method: POST\")\n",
        "            print(f\"  Request Body: {json.dumps(request_body, indent=2)}\")\n",
        "            response = requests.post(url, headers=headers, json=request_body, params=params)\n",
        "        else:\n",
        "            print(f\"  Method: GET\")\n",
        "            response = requests.get(url, headers=headers, params=params)\n",
        "        \n",
        "        response.raise_for_status()\n",
        "        \n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"âŒ Error listing logs: {e}\")\n",
        "        print(f\"   Status code: {e.response.status_code}\")\n",
        "        \n",
        "        try:\n",
        "            error_json = e.response.json()\n",
        "            print(f\"   Response: {error_json}\")\n",
        "        except:\n",
        "            print(f\"   Response (text): {e.response.text[:500]}\")\n",
        "        \n",
        "        if filters and e.response.status_code == 500:\n",
        "            print(\"\\nâš  Retrying without filters...\")\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, params=params)\n",
        "                response.raise_for_status()\n",
        "                print(\"âœ“ GET without filters succeeded\")\n",
        "            except Exception as retry_error:\n",
        "                print(f\"âŒ Retry also failed: {retry_error}\")\n",
        "                raise e\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    data = response.json()\n",
        "    results = data.get('results', [])\n",
        "    print(f\"âœ“ Found {len(results)} logs (page {page})\")\n",
        "    print(f\"  Total count: {data.get('count', 0)}\")\n",
        "    \n",
        "    if results:\n",
        "        status_counts = {}\n",
        "        for log in results:\n",
        "            status = log.get('status', 'unknown')\n",
        "            status_counts[status] = status_counts.get(status, 0) + 1\n",
        "        print(f\"  Status breakdown: {status_counts}\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def get_trace_details(exp_id: str, trace_id: str, include_full_span_tree: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"Get detailed information about a specific trace.\"\"\"\n",
        "    url = f\"{BASE_URL}/v2/experiments/{exp_id}/logs/{trace_id}/\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "    \n",
        "    params = {}\n",
        "    if include_full_span_tree:\n",
        "        params[\"detail\"] = 1\n",
        "    \n",
        "    print(f\"  Getting trace details...\")\n",
        "    print(f\"    URL: {url}\")\n",
        "    print(f\"    Method: GET\")\n",
        "    if params:\n",
        "        print(f\"    Params: {params}\")\n",
        "    \n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    return response.json()\n",
        "\n",
        "def submit_workflow_results(exp_id: str, trace_id: str, \n",
        "                           input_data: Any, output_data: Any,\n",
        "                           name: Optional[str] = None, \n",
        "                           metadata: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Update a placeholder trace with your custom workflow results.\"\"\"\n",
        "    url = f\"{BASE_URL}/v2/experiments/{exp_id}/logs/{trace_id}/\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    payload = {\n",
        "        \"input\": input_data,\n",
        "        \"output\": output_data\n",
        "    }\n",
        "    \n",
        "    if name:\n",
        "        payload[\"name\"] = name\n",
        "    if metadata:\n",
        "        payload[\"metadata\"] = metadata\n",
        "    \n",
        "    print(f\"    Submitting results to: {url}\")\n",
        "    print(f\"    Method: PATCH\")\n",
        "    print(f\"    Payload keys: {list(payload.keys())}\")\n",
        "    print(f\"    Input type: {type(input_data).__name__}\")\n",
        "    print(f\"    Output type: {type(output_data).__name__}\")\n",
        "    \n",
        "    response = requests.patch(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    updated_trace = response.json()\n",
        "    \n",
        "    print(f\"    Response status: {response.status_code}\")\n",
        "    \n",
        "    response_status = updated_trace.get('status')\n",
        "    if response_status:\n",
        "        print(f\"    Trace status: {response_status}\")\n",
        "    \n",
        "    return updated_trace\n",
        "\n",
        "def get_experiment_summary(exp_id: str, filters: Optional[List[Dict]] = None) -> Dict[str, Any]:\n",
        "    \"\"\"Get aggregated summary statistics for experiment traces.\"\"\"\n",
        "    url = f\"{BASE_URL}/v2/experiments/{exp_id}/logs/summary/\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nGetting experiment summary for {exp_id}...\")\n",
        "    print(f\"  URL: {url}\")\n",
        "    \n",
        "    if filters:\n",
        "        request_body = {\"filters\": filters}\n",
        "        print(f\"  Method: POST\")\n",
        "        print(f\"  Request Body: {json.dumps(request_body, indent=2)}\")\n",
        "        response = requests.post(url, headers=headers, json=request_body)\n",
        "    else:\n",
        "        print(f\"  Method: GET\")\n",
        "        response = requests.get(url, headers=headers)\n",
        "    \n",
        "    response.raise_for_status()\n",
        "    \n",
        "    data = response.json()\n",
        "    print(f\"âœ“ Summary retrieved:\")\n",
        "    print(f\"  Total traces: {data.get('total_count', 0)}\")\n",
        "    print(f\"  Total cost: ${data.get('total_cost', 0):.4f}\")\n",
        "    print(f\"  Total tokens: {data.get('total_tokens', 0)}\")\n",
        "    print(f\"  Avg latency: {data.get('avg_latency', 0):.2f}s\")\n",
        "    \n",
        "    return data\n",
        "\n",
        "def process_with_custom_logic(input_data: Any) -> Dict[str, Any]:\n",
        "    \"\"\"Your custom workflow processing logic.\"\"\"\n",
        "    try:\n",
        "        if isinstance(input_data, str):\n",
        "            try:\n",
        "                parsed_input = json.loads(input_data)\n",
        "            except:\n",
        "                parsed_input = input_data\n",
        "        else:\n",
        "            parsed_input = input_data\n",
        "        \n",
        "        result = {\n",
        "            \"status\": \"processed\",\n",
        "            \"message\": f\"Successfully processed input with {len(str(parsed_input))} characters\",\n",
        "            \"input_preview\": str(parsed_input)[:100],\n",
        "            \"custom_field\": \"your_custom_value\"\n",
        "        }\n",
        "        \n",
        "        return {\n",
        "            \"output\": result,\n",
        "            \"metadata\": {\n",
        "                \"processing_timestamp\": time.time(),\n",
        "                \"processor\": \"custom_workflow_v1\"\n",
        "            }\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"output\": {\n",
        "                \"status\": \"error\",\n",
        "                \"error_message\": str(e)\n",
        "            },\n",
        "            \"metadata\": {\n",
        "                \"processing_timestamp\": time.time(),\n",
        "                \"processor\": \"custom_workflow_v1\",\n",
        "                \"error\": True\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Variables\n",
        "\n",
        "These variables will track resources created throughout the workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track resources created\n",
        "log_ids = []\n",
        "dataset_id = None\n",
        "evaluator_slug = None\n",
        "experiment_id = None\n",
        "trace_ids = []\n",
        "dataset_log_count = 0\n",
        "trace_count = 0\n",
        "processed_count = 0\n",
        "traces_with_evaluators = 0\n",
        "final_status_breakdown = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create Sample Logs\n",
        "\n",
        "Create 3 sample logs that will be added to the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(1, \"Create Sample Logs\")\n",
        "\n",
        "log_examples = [\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "        ],\n",
        "        \"response\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience.\"\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain neural networks briefly.\"}\n",
        "        ],\n",
        "        \"response\": \"Neural networks are computing systems inspired by biological neural networks in animal brains.\"\n",
        "    },\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
        "        ],\n",
        "        \"response\": \"Deep learning is a type of machine learning based on artificial neural networks with multiple layers.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, example in enumerate(log_examples, 1):\n",
        "    print(f\"\\nCreating log {i}/{len(log_examples)}...\")\n",
        "    log_result = create_log(\n",
        "        model=example[\"model\"],\n",
        "        input_messages=example[\"messages\"],\n",
        "        output_message={\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
        "        custom_identifier=f\"{WORKFLOW_NAME}_log_{i}\",\n",
        "        span_name=WORKFLOW_NAME\n",
        "    )\n",
        "    log_id = log_result.get('unique_id') or log_result.get('id')\n",
        "    if log_id:\n",
        "        log_ids.append(log_id)\n",
        "        print_success(f\"Log created with ID: {log_id[:16]}...\")\n",
        "\n",
        "print_success(f\"Created {len(log_ids)} logs total\")\n",
        "print_info(\"Waiting for logs to persist in database...\")\n",
        "wait_for_processing(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Dataset from Logs\n",
        "\n",
        "Create a dataset using the logs we just created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(2, \"Create Dataset from Logs\")\n",
        "\n",
        "end_time = datetime.utcnow()\n",
        "start_time = end_time - timedelta(days=2)\n",
        "\n",
        "initial_filters = {\n",
        "    \"id\": {\n",
        "        \"value\": log_ids,\n",
        "        \"operator\": \"in\"\n",
        "    }\n",
        "}\n",
        "\n",
        "dataset_result = create_dataset(\n",
        "    name=WORKFLOW_NAME,\n",
        "    description=\"Dataset created from complete self-contained notebook\",\n",
        "    dataset_type=\"sampling\",\n",
        "    start_time=start_time.isoformat() + \"Z\",\n",
        "    end_time=end_time.isoformat() + \"Z\",\n",
        "    initial_log_filters=initial_filters\n",
        ")\n",
        "\n",
        "dataset_id = dataset_result.get('id')\n",
        "print_success(f\"Dataset created with ID: {dataset_id}\")\n",
        "print_info(\"Waiting for dataset to populate...\")\n",
        "wait_for_processing(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Verify Dataset Contains the Logs \n",
        "\n",
        "This is a critical verification step to ensure the dataset actually contains the logs we created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(3, \"Verify Dataset Contains the Logs\")\n",
        "\n",
        "dataset_logs = list_dataset_logs(dataset_id, page=1, page_size=100)\n",
        "dataset_log_count = len(dataset_logs.get('results', []))\n",
        "total_dataset_logs = dataset_logs.get('count', 0)\n",
        "\n",
        "print(f\"Expected logs: {len(log_ids)}\")\n",
        "print(f\"Dataset contains: {total_dataset_logs} logs\")\n",
        "print(f\"Retrieved for verification: {dataset_log_count} logs\")\n",
        "\n",
        "if dataset_log_count == 0:\n",
        "    print_error(\"Dataset is empty! Workflow cannot proceed.\")\n",
        "    raise Exception(\"Dataset is empty\")\n",
        "elif dataset_log_count < len(log_ids):\n",
        "    print_warning(f\"Expected {len(log_ids)} but found {dataset_log_count}\")\n",
        "    print_info(\"Some logs may not have been added yet, but proceeding...\")\n",
        "else:\n",
        "    print_success(f\"Dataset successfully populated with all {dataset_log_count} logs!\")\n",
        "\n",
        "if dataset_log_count > 0:\n",
        "    first_log = dataset_logs['results'][0]\n",
        "    print(f\"\\nðŸ“‹ Sample log from dataset:\")\n",
        "    print(f\"  ID: {first_log.get('id', 'N/A')[:24]}...\")\n",
        "    if 'input' in first_log:\n",
        "        print(f\"  Has input: Yes ({len(str(first_log['input']))} chars)\")\n",
        "    if 'output' in first_log:\n",
        "        print(f\"  Has output: Yes ({len(str(first_log['output']))} chars)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Custom Evaluator\n",
        "\n",
        "Create an LLM-based evaluator that rates response quality on a 1-5 scale based on accuracy, relevance, and completeness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(4, \"Create Custom Evaluator\")\n",
        "\n",
        "evaluator_slug = f\"custom_workflow_test_eval_{int(time.time())}\"\n",
        "\n",
        "evaluator_result = create_evaluator(\n",
        "    name=WORKFLOW_NAME,\n",
        "    evaluator_slug=evaluator_slug,\n",
        "    evaluator_type=\"llm\",\n",
        "    score_value_type=\"numerical\",\n",
        "    description=\"Evaluates response quality on a 1-5 scale\",\n",
        "    configurations={\n",
        "        \"evaluator_definition\": \"Rate the response quality based on accuracy, relevance, and completeness.\\n<llm_input>{{llm_input}}</llm_input>\\n<llm_output>{{llm_output}}</llm_output>\",\n",
        "        \"scoring_rubric\": \"1=Poor, 2=Fair, 3=Good, 4=Very Good, 5=Excellent\",\n",
        "        \"llm_engine\": \"gpt-4o-mini\",\n",
        "        \"model_options\": {\n",
        "            \"temperature\": 0.1,\n",
        "            \"max_tokens\": 200\n",
        "        },\n",
        "        \"min_score\": 1.0,\n",
        "        \"max_score\": 5.0,\n",
        "        \"passing_score\": 3.0\n",
        "    }\n",
        ")\n",
        "\n",
        "print_success(f\"Evaluator created with slug: {evaluator_slug}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Experiment with Dataset and Evaluator\n",
        "\n",
        "Create a custom workflow experiment that will create placeholder traces for each dataset entry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(5, \"Create Experiment with Dataset and Evaluator\")\n",
        "\n",
        "experiment_data = create_experiment(\n",
        "    name=WORKFLOW_NAME,\n",
        "    description=\"Testing complete self-contained workflow\",\n",
        "    dataset_id=dataset_id,\n",
        "    workflow=[{\n",
        "        \"type\": \"custom\",\n",
        "        \"config\": {\n",
        "            \"name\": WORKFLOW_NAME,\n",
        "            \"description\": \"Custom workflow from complete notebook\"\n",
        "        }\n",
        "    }],\n",
        "    evaluator_slugs=[evaluator_slug]\n",
        ")\n",
        "\n",
        "experiment_id = experiment_data.get('id')\n",
        "print_success(f\"Experiment created with ID: {experiment_id}\")\n",
        "print_info(\"Placeholder traces being created asynchronously...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Wait for Traces and List Placeholder Traces\n",
        "\n",
        "Wait for async trace creation and retrieve the placeholder traces with retry logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(6, \"Wait for Traces and List Placeholder Traces\")\n",
        "\n",
        "print_info(\"Waiting for placeholder traces (with retry)...\")\n",
        "\n",
        "trace_count = 0\n",
        "max_retries = 6\n",
        "retry_wait = 10\n",
        "\n",
        "for attempt in range(1, max_retries + 1):\n",
        "    print(f\"  Attempt {attempt}/{max_retries}...\")\n",
        "    wait_for_processing(retry_wait)\n",
        "    \n",
        "    traces_response = list_experiment_logs(experiment_id, filters=None, page_size=10)\n",
        "    trace_count = len(traces_response.get('results', []))\n",
        "    \n",
        "    if trace_count > 0:\n",
        "        print_success(f\"Found {trace_count} traces on attempt {attempt}\")\n",
        "        break\n",
        "    else:\n",
        "        if attempt < max_retries:\n",
        "            print_info(f\"No traces yet, waiting {retry_wait} more seconds...\")\n",
        "\n",
        "traces = list_experiment_logs(experiment_id, filters=None, page_size=10)\n",
        "trace_count = len(traces.get('results', []))\n",
        "\n",
        "print(f\"\\nFound {trace_count} traces\")\n",
        "\n",
        "if trace_count == 0:\n",
        "    print_error(\"No traces found after multiple retries!\")\n",
        "    raise Exception(\"No traces found\")\n",
        "\n",
        "print_success(f\"Successfully retrieved {trace_count} traces\")\n",
        "\n",
        "status_breakdown = {}\n",
        "for trace in traces.get('results', []):\n",
        "    status = trace.get('status', 'unknown')\n",
        "    status_breakdown[status] = status_breakdown.get(status, 0) + 1\n",
        "\n",
        "print(f\"\\nStatus breakdown:\")\n",
        "for status, count in status_breakdown.items():\n",
        "    print(f\"  {status}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Process Traces with Custom Workflow\n",
        "\n",
        "Process each trace with our custom logic and submit the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(7, \"Process Traces with Custom Workflow\")\n",
        "\n",
        "processed_count = 0\n",
        "max_to_process = min(trace_count, 3)\n",
        "print(max_to_process)\n",
        "for i, trace in enumerate(traces['results'][:max_to_process], 1):\n",
        "    trace_id = trace.get('id')\n",
        "    trace_ids.append(trace_id)\n",
        "    \n",
        "    print(f\"\\nProcessing trace {i}/{max_to_process} ({trace_id})...\")\n",
        "    \n",
        "    input_data = trace.get('input')\n",
        "    \n",
        "    result = process_with_custom_logic(input_data)\n",
        "    \n",
        "    updated_trace = submit_workflow_results(\n",
        "        exp_id=experiment_id,\n",
        "        trace_id=trace_id,\n",
        "        input_data=input_data,\n",
        "        output_data=result['output'],\n",
        "        name=WORKFLOW_NAME,\n",
        "        metadata=result.get('metadata')\n",
        "    )\n",
        "    \n",
        "    processed_count += 1\n",
        "    print_success(f\"Submitted results for trace {trace_id[:16]}...\")\n",
        "\n",
        "print_success(f\"Processed and submitted {processed_count} traces\")\n",
        "print_info(\"Waiting for evaluators to execute...\")\n",
        "wait_for_processing(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Verify Experiment Results \n",
        "\n",
        "Get comprehensive results including status breakdown and evaluator execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(8, \"Verify Experiment Results\")\n",
        "\n",
        "try:\n",
        "    final_summary = get_experiment_summary(experiment_id)\n",
        "    print(f\"\\nðŸ“Š Experiment Summary:\")\n",
        "    print(f\"  Total traces: {final_summary.get('total_count', 0)}\")\n",
        "    print(f\"  Total cost: ${final_summary.get('total_cost', 0):.4f}\")\n",
        "    print(f\"  Total tokens: {final_summary.get('total_tokens', 0)}\")\n",
        "    print(f\"  Avg latency: {final_summary.get('avg_latency', 0):.2f}s\")\n",
        "except Exception as e:\n",
        "    print_warning(f\"Could not get summary: {e}\")\n",
        "    final_summary = {}\n",
        "\n",
        "all_traces = list_experiment_logs(experiment_id, filters=None)\n",
        "final_status_breakdown = {}\n",
        "for trace in all_traces.get('results', []):\n",
        "    status = trace.get('status', 'unknown')\n",
        "    final_status_breakdown[status] = final_status_breakdown.get(status, 0) + 1\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Final Status Breakdown:\")\n",
        "for status, count in final_status_breakdown.items():\n",
        "    total = all_traces.get('count', 1)\n",
        "    percentage = (count / total * 100) if total > 0 else 0\n",
        "    print(f\"  {status}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ” Checking Evaluator Execution:\")\n",
        "traces_with_evaluators = 0\n",
        "\n",
        "for trace_id in trace_ids[:3]:\n",
        "    trace_detail = get_trace_details(experiment_id, trace_id, include_full_span_tree=True)\n",
        "    span_tree = trace_detail.get('span_tree', [])\n",
        "    \n",
        "    evaluator_spans = [\n",
        "        span for span in span_tree\n",
        "        if span.get('span_type') == 'SCORE' or 'evaluator' in span.get('span_name', '').lower()\n",
        "    ]\n",
        "    \n",
        "    if evaluator_spans:\n",
        "        traces_with_evaluators += 1\n",
        "        print(f\"  âœ“ Trace {trace_id[:16]}... has {len(evaluator_spans)} evaluator span(s)\")\n",
        "        for eval_span in evaluator_spans:\n",
        "            span_name = eval_span.get('span_name', 'unknown')\n",
        "            score = eval_span.get('score', 'N/A')\n",
        "            print(f\"    - {span_name}: score = {score}\")\n",
        "    else:\n",
        "        print(f\"  âš ï¸  Trace {trace_id[:16]}... has no evaluator results yet\")\n",
        "\n",
        "if traces_with_evaluators > 0:\n",
        "    print_success(f\"Evaluators executed on {traces_with_evaluators}/{len(trace_ids[:3])} sampled traces\")\n",
        "else:\n",
        "    print_warning(\"No evaluator results found (may still be processing)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Debug - View Full Span Tree Structure\n",
        "\n",
        "Let's inspect the actual span tree structure to see where the evaluator results are.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_step(9, \"Debug - View Full Span Tree Structure\")\n",
        "\n",
        "# Wait a bit more to ensure evaluators have completed\n",
        "print_info(\"Waiting 20 seconds to ensure evaluators have fully completed...\")\n",
        "wait_for_processing(20)\n",
        "\n",
        "# Get first trace with full details\n",
        "if trace_ids:\n",
        "    first_trace_id = trace_ids[0]\n",
        "    print(f\"\\nðŸ” Inspecting trace: {first_trace_id}\")\n",
        "    \n",
        "    trace_detail = get_trace_details(experiment_id, first_trace_id, include_full_span_tree=True)\n",
        "    \n",
        "    print(f\"\\nðŸ“‹ Trace-level keys: {list(trace_detail.keys())}\")\n",
        "    \n",
        "    span_tree = trace_detail.get('span_tree', [])\n",
        "    print(f\"\\nðŸŒ² Span Tree: {len(span_tree)} total spans\")\n",
        "    \n",
        "    if not span_tree:\n",
        "        print_error(\"Span tree is empty!\")\n",
        "    else:\n",
        "        for i, span in enumerate(span_tree, 1):\n",
        "            print(f\"\\n  [{i}] Span Details:\")\n",
        "            print(f\"      span_name: {span.get('span_name', 'N/A')}\")\n",
        "            print(f\"      span_type: {span.get('span_type', 'N/A')}\")\n",
        "            print(f\"      log_type: {span.get('log_type', 'N/A')}\")\n",
        "            print(f\"      status: {span.get('status', 'N/A')}\")\n",
        "            \n",
        "            # Show all keys to see what's available\n",
        "            print(f\"      Available keys: {list(span.keys())}\")\n",
        "            \n",
        "            # Check for score-related fields\n",
        "            if 'score' in span:\n",
        "                print(f\"      â­ score: {span['score']}\")\n",
        "            if 'evaluation_result' in span:\n",
        "                print(f\"      evaluation_result: {span['evaluation_result']}\")\n",
        "            if 'evaluator_slug' in span:\n",
        "                print(f\"      evaluator_slug: {span['evaluator_slug']}\")\n",
        "            \n",
        "            # Show output if it's small enough\n",
        "            output = span.get('output', '')\n",
        "            if output:\n",
        "                output_str = str(output)\n",
        "                if len(output_str) > 200:\n",
        "                    print(f\"      output: {output_str[:200]}...\")\n",
        "                else:\n",
        "                    print(f\"      output: {output_str}\")\n",
        "            \n",
        "            # Check if this span has children\n",
        "            if 'children' in span:\n",
        "                children = span.get('children', [])\n",
        "                print(f\"      children: {len(children)} child span(s)\")\n",
        "                if children:\n",
        "                    for j, child in enumerate(children, 1):\n",
        "                        print(f\"        [{j}] {child.get('span_name', 'N/A')} (type: {child.get('span_type', 'N/A')})\")\n",
        "else:\n",
        "    print_warning(\"No trace IDs available to inspect\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
